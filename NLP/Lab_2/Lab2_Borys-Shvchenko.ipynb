{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd04850",
   "metadata": {},
   "source": [
    "## First, download necessary staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bf9c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/pasha/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pasha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/pasha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/pasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa8e10",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3cf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19396e",
   "metadata": {},
   "source": [
    "## Get the actual string content of those tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8be2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2030afa",
   "metadata": {},
   "source": [
    "## Tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080aef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweets_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cb81e",
   "metadata": {},
   "source": [
    "## Let's write a function that will preprocess our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6611a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_tweets(text_tokens, del_hashtags_=True, lematize_text_=True, del_stop_words=True):\n",
    "    if del_hashtags_:\n",
    "            text_tokens = del_hashtags(text_tokens)\n",
    "    if lematize_text:\n",
    "        text_tokens = lematize_text(text_tokens)\n",
    "    if del_stop_words:\n",
    "        text_tokens = del_stopwords(text_tokens)\n",
    "        \n",
    "    return text_tokens\n",
    "\n",
    "\n",
    "def del_hashtags(text_tokens):\n",
    "    tokens = []\n",
    "    for token_ in text_tokens:\n",
    "        if \"#\" not in token_:\n",
    "            tokens.append(token_)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lematize_text(text_tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(text_tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "def del_stopwords(text_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for token, tag in pos_tag(text_tokens):\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token)):\n",
    "            continue\n",
    "        \n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134250e",
   "metadata": {},
   "source": [
    "## Let's test `preprocessing_tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbd1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", positive_tweets_tokens[50])\n",
    "print(\"After:\", preprocessing_tweets(positive_tweets_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7460015",
   "metadata": {},
   "source": [
    "## Run `preprocessing_tweets` on all positive/negative tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ccce2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = [preprocessing_tweets(tokens) for tokens in positive_tweets_tokens]\n",
    "negative_cleaned_tokens_list = [preprocessing_tweets(tokens) for tokens in negative_tweets_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c1378",
   "metadata": {},
   "source": [
    "## Let's see how did the processing go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42acf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweets_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832447f1",
   "metadata": {},
   "source": [
    "## Let's see what is most common there. Add a helper function `get_all_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40fd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    return [w for tokens in cleaned_tokens_list for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1544ede6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top',\n",
       " 'engage',\n",
       " 'member',\n",
       " 'community',\n",
       " 'week',\n",
       " ':)',\n",
       " 'hey',\n",
       " 'james',\n",
       " 'odd',\n",
       " ':/',\n",
       " 'please',\n",
       " 'call',\n",
       " 'contact',\n",
       " 'centre',\n",
       " '02392441234',\n",
       " 'able',\n",
       " 'assist',\n",
       " ':)',\n",
       " 'many',\n",
       " 'thanks']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_pos_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa4047",
   "metadata": {},
   "source": [
    "## Perform frequency analysis using `FreqDist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6958b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':)', 3691),\n",
       " (':-)', 701),\n",
       " (':d', 658),\n",
       " ('thanks', 388),\n",
       " ('follow', 357),\n",
       " ('love', 333),\n",
       " ('...', 290),\n",
       " ('good', 283),\n",
       " ('get', 264),\n",
       " ('thank', 253)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_pos.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d7217",
   "metadata": {},
   "source": [
    "## Fine. Now we'll convert these to a data structure usable for NLTK's naive Bayes classifier ([docs here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa12a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77ae5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcbc98",
   "metadata": {},
   "source": [
    "## Create two datasets for positive and negative tweets. Use 7000/3000 split for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747c7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e7a7d",
   "metadata": {},
   "source": [
    "## Finally we use the nltk's NaiveBayesClassifier on the training data we've just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0802195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2069.4 : 1.0\n",
      "                     sad = True           Negati : Positi =     25.3 : 1.0\n",
      "                followed = True           Negati : Positi =     23.7 : 1.0\n",
      "                follower = True           Positi : Negati =     21.8 : 1.0\n",
      "                     bam = True           Positi : Negati =     21.6 : 1.0\n",
      "                 welcome = True           Positi : Negati =     20.6 : 1.0\n",
      "                  arrive = True           Positi : Negati =     19.8 : 1.0\n",
      "                     x15 = True           Negati : Positi =     15.7 : 1.0\n",
      "                   didnt = True           Negati : Positi =     14.3 : 1.0\n",
      "                    glad = True           Positi : Negati =     12.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier_naive = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier_naive, test_data))\n",
    "\n",
    "print(classifier_naive.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb535e",
   "metadata": {},
   "source": [
    "## Let's check some test phrase. First, download punkt sentence tokenizer ([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25642ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = preprocessing_tweets(word_tokenize(text))\n",
    "    return classifier_naive.classify(get_token_dict(custom_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a15ea5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Negative\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Negative\n",
      "great service :  Positive\n",
      "they stole my money :  Negative\n"
     ]
    }
   ],
   "source": [
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b75fc",
   "metadata": {},
   "source": [
    "## Try to use Logistic Regression classifier instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c06d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "\n",
      "Accuracy is: 0.9943333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier_log = SklearnClassifier(LogisticRegression()).train(train_data)\n",
    "\n",
    "print(\"LogisticRegression\\n\\nAccuracy is:\", classify.accuracy(classifier_log, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb8b5a",
   "metadata": {},
   "source": [
    "## Try to use Decision Tree classifier instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a332a13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "\n",
      "Accuracy is: 0.9946666666666667\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = SklearnClassifier(tree.DecisionTreeClassifier(max_depth=5)).train(train_data)\n",
    "\n",
    "print(\"DecisionTreeClassifier\\n\\nAccuracy is:\", classify.accuracy(classifier_tree, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45260137",
   "metadata": {},
   "source": [
    "## Try to use Random Forest classifier instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4ac3bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "\n",
      "Accuracy is: 0.9953333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier_forest = SklearnClassifier(RandomForestClassifier()).train(train_data)\n",
    "\n",
    "print(\"DecisionTreeClassifier\\n\\nAccuracy is:\", classify.accuracy(classifier_forest, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffa8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
